\documentclass[parskip=full]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsthm, amssymb, mathtools, esdiff,stmaryrd, tcolorbox, bbm,float, physics}
\usepackage{tikz-cd}
\usepackage{algorithm, pdfpages}
\usepackage{bm, url, booktabs}
\usepackage[noend]{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[left=1.5cm, right=1.5cm]{geometry}
\usepackage{lipsum}

\title{Measure theory notes}
\author{jyl20 }
\date{February 2022}
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\newtheorem*{corollary}{Corollary}
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem*{proposition}{Proposition}
\newtheorem*{proofsketch}{Sketch of proof}
\newtheorem{problem}{Problem}[section]
\newtheorem*{solution}{Solution}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}
\setlength{\parindent}{0pt}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newlength{\NOTskip} 
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}
\newcommand*\interior[1]{\mathring{#1}}
\newcommand*\closure[1]{\overline{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rbar}{\overline{\R}}
\newcommand{\lsup}{\limsup_{n \to \infty}}
\newcommand{\linf}{\liminf_{n \to \infty}}
\newcommand{\ceq}{\coloneqq}
\newcommand{\limn}{\lim_{n\to \infty}}
\newcommand{\mn}{m\geq n}
\newcommand{\xseq}{(x_n)_{n \in \mathbb{N}}}
\newcommand{\setseq}{(A_n)_{n \in \mathbb{N}}}
\newcommand{\setunion}{\cup_{n \in \mathbb{N}}(A_n)}
\newcommand{\N}{\mathbb{N}}
\newcommand{\intm}[2][X]{\int_#1 #2 \, d \mu}
\newcommand{\sumlim}{\sum_{n=1}^\infty}
\newcommand{\measurablespace}{(\Omega, \mathcal{F})}
\newcommand{\measurespace}{(\Omega, \mathcal{F}, \mu)}
\newcommand{\F}{\mathcal{F}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Pcal}{\mathcal{P}} 
\newcommand{\bracketsigma}{($\sigma$-)}
\newcommand{\probabilityspace}{(\Omega,\F,\Pbb)}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\E}{\mathbb{E}}
\setlength{\parskip}{1\baselineskip plus 2pt}

\lstset{
breaklines=true,
}

\begin{document}
Throughout, let $(\Omega, \F, \Pbb)$ be a probability space.
\section{Conditional expectation}
\begin{theorem}[Existence and uniqueness of conditional expectation]
    Let $X \in L^1$, and $\mathcal{G} \subseteq \mathcal{F}$. Then there exists a random variable $Y$ such that
    \begin{itemize}
      \item $Y$ is $\mathcal{G}$-measurable
      \item $Y \in L^1$, and $\E X \mathbf{1}_A = \E Y \mathbf{1}_A$ for all $A \in \mathcal{G}$.
    \end{itemize}

    Moreover, if $Y'$ is another random variable satisfying these conditions, then $Y' = Y$ almost surely.

    We call $Y$ a (version of) the conditional expectation given $\mathcal{G}$.
\end{theorem}

\begin{proof}(Existence)

Case 1: $X \in L^2$. 

Recall that $L^2$ is a Hilbert space, and that the set of $\mathcal{G}$-measurable random variables is a closed subspace of $L^2$ (it is closed because the space $L^2(\Omega, \mathcal{G}, \Pbb)$ is complete). The projection theorem then gives us the existence and uniqueness of $Y\in L^2 \subseteq L^1$.

Case 2: $X \geq 0 \in L^1$.

Let $X_n = X \wedge n \in L^2$. Then by case 1, we can define $Y_n = \E(X_n \mid \mathcal{G}) \in L^2$. We make the following observation

\begin{lemma}
Suppose $(X, Y)$ and $(X', Y')$ are two pairs of random variables satisfying the conditions of the theorem, then $X \geq X'$ implies $Y \geq Y'$ almost surely.
\end{lemma}

\begin{proof}
    Let $A = \{Y < Y'\}$. Then $\E Y \mathbf{1}_A = \E X \mathbf{1}_A \geq \E X' \mathbf{1}_A = \E Y' \mathbf{1}_A$, so $\E (Y - Y') \mathbf{1}_A \geq 0$ and $\Pbb(A) = 0$.
\end{proof}

It follows that there is some random variable $Y$ such that $Y_n \uparrow Y$. Clearly $Y$ is $\mathcal{G}$-measurable. For any $A \in \mathcal{G}$, we have
\begin{align*}
    \E Y \mathbf{1}_A &= \lim_{n \to \infty} \E Y_n \mathbf{1}_A &&\text{(MCV)}\\
    &= \lim_{n \to \infty} \E X_n \mathbf{1}_A &&\text{(MCV)}\\
    &= \E X \mathbf{1}_A
\end{align*}

Case 3: $X \in L^1$.

Write $X = X^+ - X^-$, and apply case 2 to $X^+$ and $X^-$.

(Uniqueness)
Suppose $Y$ and $Y'$ are two random variables satisfying the conditions of the theorem. The $\{Y > Y'\}$ is in $\mathcal{G}$ so $\E Y \mathbf{1}_{\{Y > Y'\}} = \E Y' \mathbf{1}_{\{Y > Y'\}} \implies \E (Y - Y') \mathbf{1}_{\{Y > Y'\}} = 0 \implies \Pbb(Y > Y') = 0$. Similarly, $\Pbb(Y' > Y) = 0$.


\begin{remark}
The above can also be proved using the Radon-Nikodym theorem.
\end{remark}

(Proof via Radon-Nikodym)
First recall the Radon-Nikodym theorem
\begin{proposition}[Radon-Nikodym theorem]
    Let $\mu, \nu$ be two $\sigma$-finite measures on $(\Omega, \mathcal{F})$ such that $\nu \ll \mu$. Then there exists a unique (up to a.e. equivalence) $f \in L^1(\Omega, \mathcal{F}, \mu)$ such that $\nu(A) = \int_A f \, d\mu$ for all $A \in \mathcal{F}$.
\end{proposition}
Consider the measure on $(\Omega, \mathcal{G})$ given by
\[
    \mu(A) = \E X \mathbf{1}_A, \quad A \in \mathcal{G}
\]
so $\mu \ll \Pbb$. By the Radon-Nikodym theorem, there exists a unique $Y \in L^1(\Omega, \mathcal{G}, \Pbb)$ such that $\mu(A) = \int_A Y \, d\Pbb$ for all $A \in \mathcal{G}$.

For general $X \in L^1$, we can write $X = X^+ - X^-$ and apply the above to $X^+$ and $X^-$.
\end{proof}

\begin{proposition}[Equivalent definition for conditional expectation]
Let $X, \mathcal{G}$ be as above. Then there exists a random variable $Y$ such that
\begin{itemize}
    \item $Y$ is $\mathcal{G}$-measurable
    \item $Y \in L^1$ and $\E X Z = \E Y Z$ for all $Z \in L^\infty(\mathcal{G})$ 
\end{itemize}

Moreover, $Y = \E(X \mid \mathcal{G})$ almost surely.
\end{proposition}

\begin{proof}
(Existence)
Set $Y = \E(X \mid \mathcal{G})$. It is straightforward to see that $Y$ satisfies the conditions of the proposition for simple functions $Z$. Note that simple functions that are in $L^p$ are dense in $L^p$ for $1 \leq p \leq \infty$. Let $Z_n \in L^\infty(\mathcal{G})$ be a sequence of simple functions such that $Z_n \to Z$ in $L^\infty$ (in particular, we have almost sure pointwise convergence). Then
\begin{align*}
    \E X Z &= \lim_{n \to \infty} \E X Z_n &&\text{(DCT)}\\
    &= \lim_{n \to \infty} \E Y Z_n\\
    &= \E Y Z &&\text{(DCT)}
\end{align*}
\end{proof}

(Uniqueness) Note that any two random variables satisfying the conditions of the proposition are versions of the conditional expectation given $\mathcal{G}$, which was shown to be unique.

\begin{lemma}[Conditional expectation as a function]
Let $X, Y: (\Omega, \F) \to (\R, \B(\R))$. Then $Y$ is measurable with respect to $\sigma(X)$ if and only if there exists a Borel-measurable function $f: \R \to \R$ such that $Y(\omega) = f(X(\omega))$ for all $\omega \in \Omega$.
\end{lemma}


\begin{proposition}[Properties of conditional expectation]
    All (in)equality relations below hold almost surely.
    \begin{enumerate}
      \item If $X \geq 0$ a.s., then $\E(X \mid \mathcal{G}) \geq 0$
      \item If $X$ and $\mathcal{G}$ are independent, then $\E(X \mid \mathcal{G}) = \E[X]$
      \item If $\alpha, \beta \in \R$ and $X_1, X_2 \in L^1$, then
        \[
          \E(\alpha X_1 + \beta X_2 \mid \mathcal{G}) = \alpha \E(X_1 \mid\mathcal{G}) + \beta \E(X_2 \mid \mathcal{G}).
        \]
      \item \emph{Tower property}\index{tower property}: If $\mathcal{H} \subseteq \mathcal{G}$, then
        \[
          \E(\E(X \mid \mathcal{G}) \mid \mathcal{H}) = \E(X \mid \mathcal{H}).
        \]
        \item If $Z$ is bounded and $\mathcal{G}$-measurable, then
        \[
          \E(ZX \mid \mathcal{G}) = Z \E(X \mid \mathcal{G}).
        \]
      \item Let $X \in L^1$ and $\mathcal{G}, \mathcal{H} \subseteq \mathcal{F}$. Assume that $\sigma(X, \mathcal{G})$ is independent of $\mathcal{H}$. Then
        \[
          \E (X \mid \mathcal{G}) = \E(X \mid \sigma(\mathcal{G}, \mathcal{H})).
        \]
    \end{enumerate}
  \end{proposition}

\begin{proof}
    \begin{enumerate}
        \item Follows from the proof of existence and uniqueness of conditional expectation, or just use monotonicity.
        \item Let $A \in \mathcal{G}$. Then $\E( \E(X) \mathbf{1}_A) = \E X \E 1_A = \E (X 1_A)$
        \item Use linearity of conditional expectation.
        \item Let $A \in \mathcal{H}$. Then $\E\left[\E(\E(X \mid \mathcal{G}) \mid \mathcal{H}) \mathbf{1}_A \right] = \E [\E(X \mid \mathcal{G}) \mathbf{1}_A] = \E(X \mathbf{1}_A)$
        \item Easy if $Z$ is an indicator function. Then use linearity and covergence theorems.
        \item Note $\E (X \mid \mathcal{G})$ is $\sigma(\mathcal{G}, \mathcal{H})$-measurable and $\sigma(\mathcal{G}, \mathcal{H})$ is generated by the $\pi$-system $\{A \cap B: A \in \mathcal{G}, B \in \mathcal{H}\}$. We show that $\E(X \mid \mathcal{G})$ satisfies the defining property of $\E(X \mid \sigma(\mathcal{G}, \mathcal{H}))$. Let $A \in \mathcal{G}$ and $B \in \mathcal{H}$. Then for any element of the $\pi$-system, we have
        \[
               \E(\E(X \mid \mathcal{G}) \mathbf{1}_{A \cap B}) = \E [\E(X \mid \mathcal{G}) \mathbf{1}_A \mathbf{1}_B] = \E [\E(X \mathbf{1}_A \mid \mathcal{G}) \mathbf{1}_B] = \E(\underbrace{X \mathbf{1}_A}_{\in \sigma(\mathcal{G}, X)}) \E(\mathbf{1}_B) = \E(X \mathbf{1}_{A \cap B})
        \]
        Since finite measures extend uniquely from $\pi$-systems, the above holds if $A\cap B$ is replaced by any element of $\sigma(\mathcal{G}, \mathcal{H})$
    \end{enumerate}
\end{proof}

\begin{proposition}[Properties of conditional expectation]
    All (in)equality relations below hold almost surely.
    \begin{enumerate}
      \item \emph{Jensen's inequality}: If $c: \R \to \R$ is convex, then
        \[
          \E(c(X) \mid \mathcal{G}) \geq c(\E(X) \mid \mathcal{G}).
        \]
        \item For $p \geq 1$,
        \[
          \|\E(X \mid \mathcal{G})\|_p \leq \|X\|_p.
        \]
      \item \emph{Monotone convergence theorem} Suppose $X_n \uparrow X$ is a sequence of non-negative random variables. Then
        \[
          \E(X_n\mid \mathcal{G}) \uparrow \E(X \mid \mathcal{G}).
        \]
      \item \emph{Fatou's lemma}: If $X_n$ are non-negative measurable, then
        \[
          \E\left(\liminf_{n \to \infty} X_n \mid \mathcal{G}\right) \leq \liminf_{n \to \infty} \E(X_n \mid \mathcal{G}).
        \]
      \item \emph{Dominated convergence theorem}\index{dominated convergence theorem}: If $X_n \to X$ and $Y \in L^1$ such that $Y \geq |X_n|$ for all $n$, then
        \[
          \E(X_n \mid \mathcal{G}) \to \E(X \mid \mathcal{G}).
        \]
    \end{enumerate}
\end{proposition}

  \begin{proof}
    \begin{enumerate}
        \item Note that a convex function is the supremum of countably many affine functions $c(x) = \sup_{i \in I} a_i x + b_i$. Then
        \begin{align*}
            \E(c(X) \mid \mathcal{G}) &= \E \left(\sup_{i \in I}\left( a_i X + b_i \right)\mid \mathcal{G} \right)\\
            &\geq \E(a_i X + b_i \mid \mathcal{G}) \quad \forall i \in I &&\text{(monotonicity)}\\
        \end{align*} 
        So $\E(c(X) \mid \mathcal{G}) \geq \sup_{i \in I} \E(a_i X + b_i \mid \mathcal{G}) = c(\E(X \mid \mathcal{G}))$.
        \item Jensen
        \item By monotonicity, $\E(X_n \mid \mathcal{G}) \uparrow Y$ for some $Y$. By the usual monotone convergence theorem, $\E \E (X_n \mid \mathcal{G}) = \E X_n \to \E Y \leq \E X$ so $Y \in L^1$. Since each of the $\E (X_n \mid \mathcal{G})$ are $\mathcal{G}$-measurable, so is $Y$. Finally, for any $A \in \mathcal{G}$, 
        \begin{align*}
            \E Y \mathbf{1}_A &= \lim_{n \to \infty} \E \E(X_n \mid \mathcal{G}) \mathbf{1}_A &&\text{(MCV)}\\
            &= \lim_{n \to \infty} \E X_n \mathbf{1}_A\\
            &= \E X \mathbf{1}_A &&\text{(MCV)}
        \end{align*}
        \item \begin{align*}
            \E \left (\liminf_{n \to \infty} X_n \mid \mathcal{G} \right) &= \E \left (\lim_{n \to \infty} \underbrace{\inf_{m \geq n} X_m}_{increasing} \mid \mathcal{G} \right)\\
            &= \lim_{n \to \infty} \E \left (\inf_{m \geq n} X_m \mid \mathcal{G} \right) &&\text{(MCV)}\\
            &= \liminf_{n \to \infty} \E \left (\underbrace{\inf_{m \geq n} X_m}_{\leq X_n} \mid \mathcal{G}\right)\\
            &\leq \liminf_{n \to \infty}\E(X_n \mid \mathcal{G}) &&\text{(monotonicity)}
        \end{align*}
        \item Use Fatou's lemma on $Y + X_n$ and $Y - X_n$.
    \end{enumerate}
  \end{proof}

\section{Martingales}
\begin{definition}[(Discrete) stochastic process]
    A \emph{stochastic process} (in discrete time) is a collection of random variables $(X_n)_{n \in \mathbb{N}}$. A stochastic process is is \emph{integrable} if $X_n \in L^1$ for all $n$.
\end{definition}
\begin{definition}[Filtration]
    A \emph{filtration} is a sequence of $\sigma$-algebras $\mathcal{F}_n \subseteq \mathcal{F}$ such that $\mathcal{F}_n \subseteq \mathcal{F}_{n+1}$ for all $n$. We define $F_\infty = \sigma(\bigcup_{n=1}^\infty \mathcal{F}_n).$ The \emph{natural filtration} of a stochastic process $X$ is the filtration $\mathcal{F}_n = \sigma(X_1, \ldots, X_n)$. A stochastic process is \emph{adapted} to a filtration $\mathcal{F}_n$ if $X_n$ is $\mathcal{F}_n$-measurable for all $n$.
\end{definition}

\begin{definition}[Martingale]
    An integrable adapted process $(X_n)_{n \geq 0}$ is a \emph{martingale} if for all $n \geq m$, we have
    \[
      \E(X_n \mid \mathcal{F}_m) = X_m.
    \]
    We say it is a \emph{super-martingale} if
    \[
      \E(X_n \mid \mathcal{F}_m) \leq X_m,
    \]
    and a \emph{sub-martingale} if
    \[
      \E(X_n \mid \mathcal{F}_m) \geq X_m,
    \]
\end{definition}
By the tower property, it is sufficient to check the martingale property for $n = m+1$.

\begin{theorem}[Doob decomposition, non-examinable]
  Let $X_n$ be an integrable adapted process. Then there exists a martingale $M_n$ and an integrable predictable process $A_n$ such that $X_n = M_n + A_n$ and $A_0 = 0$, where predictable means that $A_n$ is $\mathcal{F}_{n-1}$-measurable for all $n \geq 1$. Moreover, $M_n$ and $A_n$ are unique up to a.s. equivalence.
\end{theorem}

\begin{proof}
(Existence)
Add up the `known' bits to get $A$ and the `surprises' to get $M$. Formally,
\begin{align*}
    A_n &= A_{n-1} + \E(X_n \mid \mathcal{F}_{n-1}) - X_{n-1}\\
    M_n &= M_{n-1} + \underbrace{X_n - \E(X_n \mid \mathcal{F}_{n-1})}_{\text{surprise}}
\end{align*}

(Uniqueness) Let $X_n = M_n + A_n = M'_n + A'_n$. Then $M_n - M'_n = A'_n - A_n$ is $\mathcal{F}_{n-1}$-measurable. But $M_n - M'_n$ is a martingale, so $\E(M_n - M'_n \mid \mathcal{F}_{n-1}) = 0$ so $M_n = M'_n$ almost surely. Similarly, $A_n = A'_n$ almost surely.
\end{proof}

\begin{definition}[Stopping time]
    A random variable $T: \Omega \to \mathbb{N} \cup \{\infty\}$ is a \emph{stopping time} if $\{T \leq n\} \in \mathcal{F}_n$ for all $n$.
\end{definition}
In the discrete case, we can equivalently require that $\{T = n\} \in \mathcal{F}_n$ for all $n$.
\begin{definition}[$X_T$]
    Let $X$ be a stochastic process and $T$ a stopping time. Then $X_T: \Omega \to \R$ is defined by cases
    \[
      X_T(\omega) = \begin{cases}
        X_n(\omega) & T(\omega) = n\\
        0 & T(\omega) = \infty
      \end{cases}
    \]
\end{definition}

\begin{definition}[Stopped $\sigma$-algebra]
    Let $T$ be a stopping time. Then the \emph{stopped $\sigma$-algebra} is
    \[
      \mathcal{F}_T = \{A \in \mathcal{F}: A \cap \{T \leq n\} \in \mathcal{F}_n \text{ for all } n\}.
    \]
\end{definition}

\begin{example}
Let $N=\text{\# of times a random walk hits -5 before it first hits 10}$ and $T$ be the first time the random walk hits 10. $N$ is $\mathcal{F}_T$-measurable
\end{example}

\begin{definition}[Stopped process]
    Let $X$ be a stochastic process and $T$ a stopping time. Then the \emph{stopped process} is $X^T_n = X_{T \wedge n}$
\end{definition}

\begin{proposition}
  $ $
  \begin{enumerate}
    \item If $T, S, (T_n)_{n \geq 0}$ are all stopping times, then
      \[
        T \vee S, T \wedge S, \sup_n T_n, \inf T_n, \limsup T_n, \liminf T_n
      \]
      are all stopping times.
    \item $\mathcal{F}_T$ is a $\sigma$-algebra
    \item If $S \leq T$, then $\mathcal{F}_S \subseteq \mathcal{F}_T$.
    \item $X_T \mathbf{1}_{T < \infty}$ is $\mathcal{F}_T$-measurable.
    \item If $(X_n)$ is an adapted process, then so is $(X^T_n)_{n \geq 0}$ for any stopping time $T$.
    \item If $(X_n)$ is an integrable process, then so is $(X^T_n)_{n \geq 0}$ for any stopping time $T$.
  \end{enumerate}
\end{proposition}

\begin{proof}
  $ $
  \begin{enumerate}
    \item Elementary
    \item Elementary
    \item Let $A \in \mathcal{F}_S$. For any $n$, we have $A \cap \{S \leq n\} \in \mathcal{F}_n$ and $A \cap \{T \leq n\} = A \cap \{S \leq n\} \cap \{T \leq n\} \in \mathcal{F}_n$.
    \item $X_T \mathbf{1}_{T < \infty} = \sum_{n=1}^\infty X_n \mathbf{1}_{\{T = n\}}$ where each of the terms is $\mathcal{F}_T$-measurable.
    \item $X^T_n = X_n \mathbf{1}_{\{T \geq n\}} + X_T \mathbf{1}_{\{T < n\}} \mathbf{1}_{\{T < \infty\}}$.
    \item $X^T_n = X_n \mathbf{1}_{\{T \geq n\}} + \sum _{k=1}^{n-1} X_k \mathbf{1}_{\{T = k\}}$ so $E |X^T_n| \leq E|X_n| + \sum_{k=1}^{n-1} E|X_k| < \infty$.
  \end{enumerate}
\end{proof}

\begin{theorem}[Equivalent definitions for super-martingales]
  Let $(X_n)_{n \geq 0}$ be an integrable and adapted process. Then the following are equivalent:
  \begin{enumerate}
    \item $(X_n)_{n \geq 0}$ is a super-martingale.
    \item For any bounded stopping times $T$ and any stopping time $S$,
      \[
        \E(X_T \mid \mathcal{F}_S) \leq X_{S \wedge T}.
      \]
    \item $(X_n^T)$ is a super-martingale for any stopping time $T$.
    \item \label{super martingale: monotone} For bounded stopping times $S, T$ such that $S \leq T$, we have
      \[
        \E X_T \leq \E X_S.
      \]
  \end{enumerate}
\end{theorem}

\begin{proof}
  \begin{itemize}
    \item[--] $(2) \Rightarrow (1)$: Let $n \geq m$ and set $T = n, S = m$. 
    \item[--] $(2) \Rightarrow (4)$: Tower rule
    \item[--] $(2) \Rightarrow (3)$: Let $n \geq m$
    \[
      \E(X_n^T \mid \mathcal{F}_m) = \E (X_{T \wedge n} \mid \mathcal{F}_m) \leq X_{T \wedge m \wedge n} = X_m^T.
    \]
    \item[--] $(1) \Rightarrow (2)$ Let $T \leq N$
    \begin{align*}
      X_T &=X_{S \wedge T} + \sum_{k = 0}^N (X_{k + 1} - X_k) \mathbf{1}_{S \leq k < T} \tag{$*$}
    \end{align*}
    Let $A \in \mathcal{F}_S$.
    \begin{align*}
      \E \left[(X_{k + 1} - X_k)\mathbf{1}_{S \leq k < T} \mathbf{1}_A \right] &= \E \left[\E \left[(X_{k + 1} - X_k)\underbrace{\mathbf{1}_{S \leq k < T}\mathbf{1}_A}_{\in \mathcal{F}_k} \mid \mathcal{F}_k \right]\right] \\
      &= \E \left[ \mathbf{1}_{S \leq k < T} \mathbf{1}_A \underbrace{\E \left[(X_{k + 1} - X_k) \mid \mathcal{F}_k \right]}_{\leq 0}\right] \\
      &\leq 0
    \end{align*}
    so $\E X_T \mathbf{1}_A \leq \E X_{S \wedge T} \mathbf{1}_A$. By Radon-Nikodym, $\E(X_{S \wedge T} - X_T \mid \mathcal{F}_S) \geq 0$. But $X_{S \wedge T}$ is $\mathcal{F}_S$-measurable, so $X_{S \wedge T} - X_T \geq 0$ almost surely.

    \item[--] $(4) \Rightarrow (2)$ Let $n \geq m$ and $A \in \mathcal{F}_m$. One can check that $T = m \mathbf{1}_A + n \mathbf{1}_{A^c} \leq n$ is a stopping time such that
    \begin{align*}
      \E((X_n - X_m) \mathbf{1}_A) &= \E(X_n - X_T) \leq 0 \\
    \end{align*}
    By Radon-Nikodym, $\E(X_m - X_n \mid \mathcal{F}_m) \geq 0$ so $\E(X_n \mid \mathcal{F}_m) \leq X_m$.

    \item [--] $(3) \Rightarrow (1)$ Let $T = \infty$
  \end{itemize}
\end{proof}

\begin{theorem}[Optional stopping]
  Let $(X_n)_{n \geq 0}$ be a martingale and $T$ a stopping time. Then $E(X_T) = E(X_0)$ if any of the following conditions hold:
  \begin{enumerate}
    \item $T$ is almost surely bounded, i.e. there is some $N$ such that $T \leq N$ almost surely.
    \item $X$ has bounded increments, i.e. there is some $K$ such that $|X_{n+1} - X_n| \leq K$ for all $n$ almost surely and $T$ is integrable
    \item There exists an integrable random variable $Y$ such that $|X_n| \leq Y$ for all $n$ almost surely and $T$ is finite almost surely, i.e. $\Pbb(T < \infty) = 1$.
  \end{enumerate}
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \item Use (\ref{super martingale: monotone}) of the previous theorem with $S = 0$, or prove directly.
    \item placeholder
    \item placeholder
  \end{enumerate}
\end{proof}
\end{document}
